# Inference in MLR {#mlrinference}

<!--- For HTML Only --->
`r if (!knitr:::is_latex_output()) '
$\\newcommand{\\E}{\\mathrm{E}}$
$\\newcommand{\\Var}{\\mathrm{Var}}$
$\\newcommand{\\bmx}{\\mathbf{x}}$
$\\newcommand{\\bmH}{\\mathbf{H}}$
$\\newcommand{\\bmI}{\\mathbf{I}}$
$\\newcommand{\\bmX}{\\mathbf{X}}$
$\\newcommand{\\bmy}{\\mathbf{y}}$
$\\newcommand{\\bmY}{\\mathbf{Y}}$
$\\newcommand{\\bmbeta}{\\bm{\\beta}}$
$\\newcommand{\\XtX}{\\bmX^\\mT\\bmX}$
$\\newcommand{\\mT}{\\mathsf{T}}$
$\\newcommand{\\XtXinv}{(\\bmX^\\mT\\bmX)^{-1}}$
'`

```{r include=FALSE}
library(tidyverse)
library(broom)
```

## What kind of hypothesis test?

In multiple linear regression, there are three types of questions we could ask about the importance of the predictor variables. They differ by whether they involve one predictor, all predictors, or a subset of predictors.


**Single Variable**  

* What is the importance of a specific predictor?  
* This question can be addressed with a T-test (Section \@ref(mlrttest).
* Examples 
  * What is the relationship, if any, between average air pollution levels and cardiovascular mortality rates, after controlling for temperature?
  * What is the relationship, if any, between gender and salary, controlling for years of experience and position title?
  * What is the relationship, if any, between marijuana use and opioid overdoses, controlling for socioeconomic status?
  * What is the relationship, if any, between hours spent on homework and final exam score, controlling for class level and department?


**All Variables** 

* What is the overall importance of the model?   
* This question can be addressed with a global F-test (Section \@ref(mlrftest).
* Examples:
  * How well do temperature and air pollution explain variation in cardiovascular mortality rates?
  * Do gender, years of experience, and position title explain differences in salary?
  * Can rates of opioid overdoses be explained by rates of marijuana use and socioeconomic status?
  * Can we predict final exam score using time spent on homework, class level, and department?

**Subset of Variables** 

* Which subsets of predictors are important?
* This question can be addressed with a partial F-test  (Section \@ref(mlrpartialftest). 
* Examples:
    * How well do temperature and air pollution explain variation in cardiovascular mortality rates?
    * Do gender, years of experience, and position title explain differences in salary?
    * Can rates of opioid overdoses be explained by rates of marijuana use and socioeconomic status?
    * Can we predict final exam score using time spent on homework, class level, and department?

<!-- Questions we can ask about our model: -->

<!-- 1. What is the importance of a specific predictor? (T-test) -->
<!-- 2. What is the overall importance of the model? (F-test) -->
<!-- 3. Which subsets of predictors are important? (Partial F-tests) -->

<!-- Question 1 Examples:  -->
<!-- Question 2 Examples: -->
<!-- We will first consider Question 1 (specific predictors), and then turn to Question 2 (overall significance) and Question 3 (groups of predictors). -->


## Photosynthesis Data

For examples in this chapter, we will use data on photosynthesis output in trees from Reich *et al.*, *Nature*, 2018.^[Reich, P.B., A. Stefanski, K.M. Sendall, and R.L. Rich. 2018. Photosynthetic data on experimentally warmed tree species in northern Minnesota, 2009-2011, used in the paper Reich et al Nature 2018. ver 2. Environmental Data Initiative. https://doi.org/10.6073/pasta/258239f68244c959de0f97c922ac313f] They measured photosynthesis output under different conditions, including variations in the amount of water in the soil and temperature in the surrounding air.

We can fit a multiple linear regression model for photosynthesis output ($Y$) that adjusts for soil moisture content ($x_1$), an indicator of whether the tree was artificially warmed ($x_2$),  and leaf temperature ($x_3$).^[For this example, we are ignoring some features of the design such as correlation by plot.] 


```{r eval=TRUE, echo=FALSE,  include=FALSE, message="hide", size="footnotesize"}
photo <- read_csv("data/photo.csv", col_types = cols())
photo <- subset(photo,
                   !is.na(soil_water) & !is.na(warming_treatment) &
                  !is.na(tleaf))
head(photo)
```

```{r eval=FALSE, echo=F, message="hide", warning=FALSE, fig.height=6, fig.width=8}
g1 <- ggplot(photo) +
  geom_point(aes(x=soil_water, y=photosyn, col=warming_treatment)) + 
  scale_color_discrete(name="Warming Treatment: ") + xlab("Soil Water Content Ratio") + ylab(expression("Photosynthesis Output ("~mu~"mol"/m^2/s~")"))
g2 <- ggplot(photo) +
  geom_point(aes(x=tleaf, y=photosyn, col=warming_treatment)) + 
  xlab("Leaf Temperature (deg C)") + ylab(expression("Photosynthesis Output ("~mu~"mol"/m^2/s~")"))
g12 <- plot_grid(g1 + theme(legend.position="none"),
                          g2 + theme(legend.position="none"),
                          labels = c("", ""),
                 ncol=2)
legend_b <- get_legend(g1 + theme(legend.position="bottom",legend.justification="center"))
plot_grid( g12, legend_b, nrow = 2, rel_heights = c(1, .2), align="hv")
```


```{r eval=TRUE, echo=TRUE}
ph_lm <- lm(photosyn~soil_water + warming_treatment  + tleaf,
            data=photo)
```

We obtain the fitted model:

\begin{equation}
\hat y = 3.89 + 40.5x_{1} + 1.4x_{2} -0.022x_3
(\#eq:photofitted)
\end{equation}


```{r eval=FALSE, echo=FALSE}
ph_lm <- lm(photosyn~soil_water + warming_treatment  + tleaf,
            data=photo)
summary(ph_lm)
```


<!-- ### Hypothesis Tests for $\beta_1$ in Photosynthesis data -->


## Hypothesis Tests for $\beta_j$ {#mlrttest}


### Scientific vs. Statistical Question
Using this model \@ref(eq:photofitted),  we could ask the **scientific question:**

Is there a relationship between soil water content ratio and photosynthesis output, *after adjusting for leaf temperature and warming treatment*?

To translate this into a statistical question, we need to isolate what represents the relationship between soil water content ratio and photosynthesis output. This is precisely $\beta_1$, since it represents the slope of the relationship between soil water content ($x_1)$ and average photosynthesis outtput ($E[Y]$), for trees with the same value of leaf temperature and warming treatment.
Thus, our corresponding **statistical question** is:

Is $\beta_{1}\ne 0$ in this model?


### General Form of $H_0$
The standard setup for a hypothesis test of a single coefficient parameter in MLR is similar to SLR. We consider null and alternative hypotheses of the form
\begin{equation}
H_0: \beta_j = \beta_{j0} \quad \text{vs.} \quad H_A: \beta_j \ne \beta_{j0}
(\#eq:mlrH0)
\end{equation}
One-sided hypotheses are also possible, although less common than the two-sided versions.
To test the null hypothesis in \@ref(eq:mlrH0), we use the $t$-statistic that has the same form as the one from SLR:
$$t = \frac{\hat\beta_j - \beta_{j0}}{\widehat{se}(\hat\beta_j)}$$
However unlike in SLR, the standard error in the denominator,  $se(\hat\beta_j) = \sqrt{\hat\sigma^2(\XtX)^{-1}_{jj}}$,  depends on $x_j$ and all of the other $x$'s. This means that the correlation between predictor variables can impact the results of the hypothesis test (and the width of confidence intervals).

When $H_0$ is true, $t$ follows a T-distribution with $n-p$ degrees of freedom. The corresponding $p$-value is computed in the usual way: $p = P(T> |t|)$. The $t$ statistic and $p$-value provided by R in standard output correspond to $\beta_{j0}=0$.



```{example}
In the photosynthesis data, is there evidence of a relationship between soil water content ratio and average photosynthesis output, *after adjusting for leaf temperature and warming treatment*?
```

To answer this, we compute the test statistic and obtain:
$$t = \frac{\hat\beta_1 - \beta_{10}}{se(\hat\beta_1)} = \frac{40.5 - 0}{2.84} = 14.24$$
The corresponding $p$-value is:
$$P(|T_{1311}| > |14.24|) < 0.0001$$
Thus, we reject the null hypothesis that $\beta_1 = 0$ and conclude that there is a linear relationship between soil water content and photosynthesis output, when adjusting for warming treatment and leaf temperature.

All of the information necessary to conduct this hypothesis test is availble in the standard summaries of an `lm` object in R. For example:

```{r}
tidy(ph_lm)
```





## Confidence Intervals for $\beta_j$


Confidence intervals for the $\beta$ parameters have the same form as in SLR:
\begin{equation}
(\hat\beta_j - t_{1-\alpha/2}\widehat{se}(\hat\beta_j), \hat\beta_j + t_{1-\alpha/2}\widehat{se}(\hat\beta_j))
(\#eq:mlrCI)
\end{equation}
where $t_{1-\alpha/2}$ is such that $P(T_{n-p} < t_{1-\alpha/2}) = 1-\alpha/2$.
The interval in \@ref(eq:mlrCI) is a random interval that, assuming the model is correct, includes the true value of the parameter $\beta_j$ with probability (1-$\alpha$).

The same functions that provide confidence intervals in R for SLR (`confint()` and `broom::tidy()`) provide them for models with multiple variables. If desired, the intervals can also be constructed from the individual components in  \@ref(eq:mlrCI).

```{example}
To construct a confidence interval for $\beta_1$ in \@ref(eq:photofitted), we first compute the necessary elements:
```

* $\hat\beta_1 = 40.5$
* $t_{1 - \alpha/2} = 1.96$
* $\widehat{se}(\hat\beta_1) = 2.84$

We then compute the interval:

$$(40.5 - 1.96*2.84, 40.5 + 1.96*2.84) = (34.9, 46.1)$$

In R, we could accomplish this using:

```{r eval=FALSE, echo=TRUE}
t_alphaOver2 <- qt(p=0.975, df=nobs(ph_lm))
b1hat <- coef(ph_lm)[2]
seb1hat <- sqrt(diag(vcov(ph_lm)))[2]
c(Lower=b1hat - t_alphaOver2*seb1hat,
  Upper=b1hat + t_alphaOver2*seb1hat)
```


But in practice, it is much faster and simpler to use `tidy()`:
```{r eval=TRUE, echo=TRUE}
tidy(ph_lm, conf.int=TRUE, conf.level=0.95)
```


## Testing for Significance of Regression {#mlrftest}

### Testing for Significance of Regression (Global F-Test)

**Question:** Is there any linear relationship between the predictor variables and photosynthesis output?

This is a "**Global F-test**":
\begin{align*}
H_0:& \beta_1 = \beta_2 = \beta_3 = 0 \\
H_A:& \beta_1 \ne 0 \text{ and/or }\beta_2 \ne 0 \text{ and/or } \beta_3 \ne 0
\end{align*}

$H_0$: There is no linear relationship between average photosynthesis output and soil water content ratio, tree warming status, and leaf temperature.  
$H_A$: There is a linear relationship between average photosynthesis output and soil water content ratio, tree warming status, and leaf temperature.


The general form of the Global F-Test is:
\begin{align*}
H_0:& \beta_1 = \beta_2 = \cdots = \beta_k = 0 \\
H_A:& \beta_j \ne 0 \text{ for at least one \textit{j}}
\end{align*}


**Note:** $H_A$ does not specify *which* coefficient is non-zero, only that *at least 1* is non-zero.


How do we test this hypothesis?  

In SLR:

* Sum of squares decomposition: $SS_{Tot} = SS_{Reg} + SS_{Res}$
* $f = MS_{Reg}/MS_{Res} = \dfrac{SS_{Reg}/df_{Reg}}{SS_{Res}/df_{res}}$
  * $MS_{Res}$ is average residual variation ($\hat\sigma^2$)
  * $MS_{Reg}$ is average amount of variability explained by each predictor (SLR had only 1).

We will use the same approach for MLR.

Sum of squares decomposition is the same:
\begin{align*}
SS_{Tot} &= \sum_{i=1}^n (y_i - \overline{y})^2\\
SS_{Reg} &= \sum_{i=1}^n (\hat y_i - \overline{y})^2\\
SS_{Res} &= \sum_{i=1}^n (y_i - \hat y_i)^2\\
SS_{Tot} &= SS_{Reg} + SS_{Res}
\end{align*}

### F-statistic

The F statistic is 
$$f = \frac{SS_{Reg} / df_{Reg}}{SS_{Res}/ df_{Res}} \approx \frac{\text{Signal}}{\text{Noise}}.$$

What about degrees of freedom?

* $df_{Reg} = k$ (**not $p$!**). This is the number of predictors, not counting intercept
* $df_{Res} = n - p = n - (k + 1) = n - k - 1$.


When $H_0$ is true (and assuming approximate normality), $f$ follows a $F_{df_{Reg}, df_{Res}}$ distribution. Reject if $f$ is large enough.


### Testing for Significance of Regression (Global F-Test)


We summarize this in an ANOVA table:

\begin{tabular}{l cccc}
\hline
Source of & Sum of & Degrees of \\
Variation & Squares &   Freedom & MS & F \\
\hline
Regression & $SS_{reg}$ & $k$ & $MS_{reg}$ & $MS_{reg}/MS_{res}$ \\
Residual & $SS_{res}$ & $n-(k+1) = n- p$ & $MS_{res}$ & -- \\
Total & $SS_{tot}$ & $n-1$ & -- & -- \\
\hline
\end{tabular}


\vspace{0.8cm}

R provides the $F$ statistic and $p$-value for a test of significance of regression. This is provided at the bottom of \texttt{summary()} output.


## Example: Photosynthesis Data


```{r echo=TRUE, size="footnotesize", output.lines=-1:-9}
summary(ph_lm)
```

Conclusion:

\vspace{5cm}

### Photosynthesis Example -- F-test "by hand"

```{r echo=TRUE}
SSres <- sum(residuals(ph_lm)^2)
SStot <- sum((photo$photosyn- mean(photo$photosyn))^2)
SSreg <- SStot - SSres
f <- (SSreg/3)/(SSres/(nobs(ph_lm)-4))
f
pf(f, df1=3, df2=nobs(ph_lm)-4, lower=F)
```

\vspace{0.5cm}
Warning: If doing this "by hand", be careful about missing values.


## $R_{Adj}^2$ and Testing for Subsets of Coefficients {#adjr2}
### Testing for Subsets of Coefficients

What if we want to test whether adding/removing a group of variables matters?

\begin{align*}
\text{Photosyn. Output} &= \text{Soil WC}\\
\text{Photosyn. Output} &= \text{Soil WC + Warming Treatment + Leaf Temp.}
\end{align*}

Can we use $R^2$ to compare the two models?  

* $R^2 = \dfrac{SS_{Reg}}{SS_{Tot}} = 1 -\dfrac{SS_{Res}}{SS_{Tot}}$ 
* The percent of variability explained by the model.


### Adjusted $R^2$

**Problem:** $R^2$ will always go up when you add a variable.  

*  Adding a variable with true value $\beta =0$ will still have $\hat\beta \ne 0$ (although maybe very close). 
* This means that $SS_{Res}$ will be (slightly) smaller, meaning $SS_{Reg}$ goes up.
* So the ratio $SS_{Reg}/SS_{Tot}$ will increase $\Rightarrow$ higher $R^2$

\vspace{0.8cm}

**$R^2$ should not be used to compare models**.  

Adjusted $R^2$ is one solution:

\begin{align*}
R^2 &= 1 - \frac{SS_{Res}}{SS_{Tot}}\\
R^2_{Adj} &= 1 - \frac{SS_{Res}/(n - p)}{SS_{Tot}/(n-1)}
\end{align*}

* This provides a penalty as $p$ (or $k$) increases
* If a variable is added that does not reduce $SS_{Res}$ much, then $R^2_{Adj}$ goes down
* As $n \to \infty$, $R^2_{Adj} \to R^2$

### Testing for Subsets of Coefficients {#mlrpartialftest}


Let's now do a formal statistical test to compare:

\begin{align*}
\text{Photosyn. Output} &= \text{Soil WC}\\
\text{Photosyn. Output} &= \text{Soil WC + Warming Treatment + Leaf Temp.}
\end{align*}


### Testing for Subsets of Coefficients


$$Y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3} + \epsilon_i$$

\small
\begin{align*}
Y &= \text{ Photosynthesis output (variable \texttt{photosyn})}\\
x_1 &= \text{ Soil Water Content Ratio (variable \texttt{soil\_water})} \\
x_2 &= \text{ Tree Warmed Indicator (variable \texttt{warming\_treatment};}\\
& \qquad \qquad \text{1 = \texttt{warmed}, 0 = \texttt{ambient})}\\
x_3 &= \text{ Leaf Temperature in Degrees Celsius (variable \texttt{tleaf})}
\end{align*}

\begin{align*}
H_0:& \beta_2 = \beta_3 = 0\\
H_A:& \beta_2 \ne 0 \text{ and/or } \beta_3 \ne 0
\end{align*}

### Partial F-Test

Test this null hypothesis using a **partial F-Test**

* Fit the "full" model and "reduced" model
  * Full model corresponds to $H_A$
  * Reduced model corresponds to $H_0$
* Use difference in $SS_{Reg}$ between full and reduced model to compute $f$ statistic


Full model:
$$Y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3} + \epsilon_i$$
Reduced model:
$$Y_i = \beta_0 + \beta_1x_{i1} + \epsilon_i$$

* $SS_{reg}^{Full}$ = Variation in outcome explained by full model
* $SS_{reg}^{Red}$ = Variation in outcome explained by reduced model
* Note that $SS_{reg}^{Full} > SS_{reg}^{Red}$
* $SS_{reg}^{Full} - SS_{reg}^{Red}$ is the "extra sum of squares" explained by the full model compared to the reduced model
* Number of parameters set to zero in reduced model: $r = 2$


Test statistic is:

$$f = \dfrac{\left(SS_{reg}^{Full} - SS_{reg}^{Red}\right)/ r}{SS_{Res}/(n - p)}$$

* $SS_{Res}$ is calculated from full model
* $r$ is number of parameters set to 0 in reduced model
* Compare to $F_{r, n-p}$ distribution to obtain $p$-value.


### Partial F-Test -- Matrix form


Setup: Partition the $\bmbeta$ vector into two parts $A$ and $B$:
$$\bmbeta = \begin{bmatrix} \beta_1 \\ \beta_2 \\ \vdots \\ \beta_{k-r} \\ \beta_{k-r+1} \\ \vdots \\ \beta_k\end{bmatrix} = \begin{bmatrix} \bmbeta_A \\ \bmbeta_B \end{bmatrix}$$

<!-- ### Partial F-Test -- Matrix form -->

Similarly, partition $\bmX$ into two parts:
$$\bmX = \begin{bmatrix} 1 & x_{11} & \cdots & x_{1,k-r} & x_{1, k-r + 1} & \cdots & x_{1k} \\ \vdots & \vdots & & \vdots & \vdots & & \vdots \\ 1 & x_{n1} & \cdots & x_{n,k-r} & x_{n, k-r + 1} & \cdots & x_{nk} \end{bmatrix} = \begin{bmatrix} \bmX_A & \bmX_B \end{bmatrix}$$

The Full Model is: $\bmy = \bmX\bmbeta + \bmepsilon$  
The Reduced Model is: $\bmy = \bmX_A\bmbeta_A + \bmepsilon$

\vspace{0.5cm}
\footnotesize
\begin{tabular}{l  c c}
\hline
 & Full Model & Reduced Model \\
 \hline
Model & $\bmy = \bmX\bmbeta + \bmepsilon$ & $\bmy = \bmX_A\bmbeta_A + \bmepsilon$ \\
$\hat\bmbeta$ & $\hat\bmbeta = (\bmX^\mT\bmX)^{-1}\bmX^\mT\bmy$ & $\hat\bmbeta_A = (\bmX_A^\mT\bmX_A)^{-1}\bmX_A^\mT\bmy$ \\
$SS_{Res}$ & $SS_{Res}(\bmbeta) = (\bmy - \bmX\hat\bmbeta)^\mT(\bmy - \bmX\hat\bmbeta)$ & $SS_{Res}(\bmbeta_A) = (\bmy - \bmX_A\hat\bmbeta_A)^\mT(\bmy - \bmX_A\hat\bmbeta_A)$ \\
$SS_{Reg}$ & $SS_{Reg}(\bmbeta) = SS_{Tot} - SS_{Res}(\bmbeta)$ & $SS_{Reg}(\bmbeta_A) = SS_{Tot} - SS_{Res}(\bmbeta_A)$\\
\hline
\end{tabular}
\vspace{0.5cm}

\normalsize

$SS_{Reg}(\bmbeta_B | \bmbeta_A) = SS_{Reg}(\bmbeta) - SS_{Reg}(\bmbeta_A)$ is the "extra sum of squares" due to $\bmbeta_B$.

### Partial F-Test -- Matrix form

$$H_0: \bmbeta_B = \mathbf{0} \text{ vs. } H_A: \bmbeta_B \ne \mathbf{0}$$

$$f = \dfrac{SS_{reg}(\bmbeta_B | \bmbeta_A) / r}{SS_{Res}/(n - p)}$$

* If $H_0$ is true ($\bmbeta_B = \mathbf{0}$), then $f \sim F_{r,n-p}$. 
* Reject $H_0$ if $f$ is too large.


## Partial F-Test in R

To perform partial F-test in R:

* Fit the full model
* Fit the reduced model
* Use `anova(reduced_mod, full_mod)`

```{r echo=TRUE, size="footnotesize"}
ph_lm <- lm(photosyn~soil_water + warming_treatment  + tleaf,
            data=photo)
ph_lm_reduced <- lm(photosyn~soil_water,
            data=photo)
```



```{r echo=TRUE, size="footnotesize"}
anova(ph_lm_reduced, ph_lm)
```
\vspace{0.5cm}

* The column `RSS` gives $SS_{res}^{Red}$ and $SS_{res}^{Full}$
* $SS_{res}^{Red} - SS_{res}^{Full} = SS_{reg}^{Full} - SS_{reg}^{Red}$


### Photosynthesis Example

Conclusion:  

We reject $H_0$ and conclude that there is a linear relationship between average photosynthesis output and warming treatment and leaf temperature  in a model that already contains soil water content.

### Photosynthesis Example II

$$Y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \beta_3x_{i3} + \epsilon_i$$

\small
\begin{align*}
Y &= \text{ Photosynthesis output (variable \texttt{photosyn})}\\
x_1 &= \text{ Soil Water Content Ratio (variable \texttt{soil\_water})} \\
x_2 &= \text{ Tree Warmed Indicator (variable \texttt{warming\_treatment};}\\
& \qquad \qquad \text{1 = \texttt{warmed}, 0 = \texttt{ambient})}\\
x_3 &= \text{ Leaf Temperature in Degrees Celsius (variable \texttt{tleaf})}
\end{align*}

\begin{align*}
H_0:& \beta_3 = 0\\
H_A:& \beta_3 \ne 0
\end{align*}

Partial F-test with one variable is equivalent to t-test  

```{r echo=TRUE, size="footnotesize", output.lines=-1:-2}
ph_lm_reduced2 <- lm(photosyn~soil_water + warming_treatment, data=photo)
anova(ph_lm_reduced2, ph_lm)
```


```{r echo=TRUE, size="footnotesize"}
tidy(ph_lm)
```


## Confidence Intervals for Mean

### Confidence Intervals for Mean

Suppose we want to estimate the mean photosynthesis output for a tree with predictor variables
$$\bmx_0 = \begin{bmatrix} 1 \\ 0.15 \\ 0 \\ 22 \end{bmatrix}$$ 


What do these values mean?

\vspace{2cm}


Our best estimate of the mean is given by: 

$$\hat\mu_0 = \bmx_0^\mT\hat \bmbeta = 1\times\hat\beta_0 + 0.15\times\hat\beta_1 + 0 \times \hat\beta_2 + 22\times\hat\beta_3$$

Use a confidence interval to represent uncertainty.

Need to know the standard error (or variance) of $\hat\mu_0$.

\begin{align*}
\Var(\hat \mu_0) & = \Var(\bmx_0^\mT\hat\bmbeta) \\
&= \bmx_0^\mT \Var(\hat\bmbeta)\bmx_0 \\
&= \bmx_0^\mT\left(\sigma^2 (\bmX^\mT\bmX)^{-1}\right)\bmx_0 \\
\end{align*}


We estimate $\Var(\hat \mu_0)$ with: 
$$\widehat{\Var}(\hat \mu_0) = \bmx_0^\mT\left(\hat\sigma^2 (\bmX^\mT\bmX)^{-1}\right)\bmx_0.$$

Standard error is the square-root: 
$$\hat{se}(\hat\mu_0) = \sqrt{\bmx_0^\mT\left(\hat\sigma^2 (\bmX^\mT\bmX)^{-1}\right)\bmx_0.}$$


The CI is:
$$(\hat\mu_0 - t_{\alpha/2}\hat{se}(\hat\mu_0), \hat\mu_0 + t_{\alpha/2}\hat{se}(\hat\mu_0))$$


* $df$ for $t_{\alpha/2}$ is $n-p$

### Confidence Intervals for Mean in R

* Create a new data frame with specified values of $\bmx_0$
* Use \texttt{predict()} to compute $\hat\mu$ and CI

```{r echo=TRUE}
pred_data <- data.frame(soil_water=0.15,
                        warming_treatment="ambient",
                        tleaf=22)
pred_data
```

<!-- ### Confidence Intervals for Mean in R -->


```{r echo=TRUE}
predict(ph_lm, newdata=pred_data,
        interval="confidence")
```


```{r echo=TRUE}
pred_data2 <- data.frame(soil_water=c(0.15, 0.2),
                        warming_treatment=c("ambient",
                                            "warmed"),
                        tleaf=c(22, 19))
pred_data2
predict(ph_lm, newdata=pred_data2,
        interval="confidence")
```

## Prediction Intervals for New Observations

Suppose we want to predict the photosynthesis output for a new **observation** of a tree with predictor variables 
$$\bmx_0 = \begin{bmatrix} 1 \\ 0.15 \\ 0 \\ 22 \end{bmatrix}$$

Our best prediction is the same: 
$$\hat y_0 = \bmx_0^\mT\hat\bmbeta$$

The variance of the difference between this value and the true observation ($y$) is: $$\Var(\hat y_0 - y_0) = \bmx_0^\mT\left(\sigma^2 (\bmX^\mT\bmX)^{-1}\right)\bmx_0 + \sigma^2 = \sigma^2 \left[\bmx_0^\mT\left( (\bmX^\mT\bmX)^{-1}\right)\bmx_0 + 1\right].$$



Uncertainty is quantified using a prediction interval:
$$(\hat y_0 - t_{\alpha/2}\sqrt{\sigma^2 \left[\bmx_0^\mT\left( (\bmX^\mT\bmX)^{-1}\right)\bmx_0 + 1\right]}, \hat y_0 + t_{\alpha/2}\sqrt{\sigma^2 \left[\bmx_0^\mT\left( (\bmX^\mT\bmX)^{-1}\right)\bmx_0 + 1\right]})$$


### Prediction Intervals in R

```{r echo=TRUE}
predict(ph_lm, newdata=pred_data, interval="prediction")
predict(ph_lm, newdata=pred_data2, interval="prediction")
```



## Exercises

```{exercise photo-beta2-inference}
In model \@ref(eq:photofitted), state in words what the null hypothesis $H_0: \beta_2 = 0$ means. Conduct the test at the $\alpha=0.05$ level and summarize your conclusiosn in one or two sentences.
```

```{exercise}
Calculate a 95\% confidence interval for $\beta_3$ in \@ref(eq:photofitted). Use the interval to test $H_0: \beta_3 = 0$ at the $\alpha=0.05$ level.
```




